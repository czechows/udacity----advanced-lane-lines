\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{tikz}

\begin{document}

\pagestyle{empty}
\noindent

\title{Udacity CarND Advanced Lane Finding project}
\author{Aleksander Czechowski}
\maketitle

The purpose of this project was to implement an advanced lane finding algorithm,
that would take into account the curvature of the lane (as opposed to fitting straight lines in one of the previous projects).
The main goal of the project was to reliably detect the lanes in the video project\_video.mp4.
The objective was achieved, as documented in the video output.mp4.
All the relevant code is contained in the notebook ALF.ipynb.

\section{Rubric points}

\subsection{Camera calibration}

\subsubsection{Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.}


The code for this step is contained in the functions \emph{camera\_calib} and \emph{undistort}.
In the function \emph{camera\_calib} I first generate the ``object points'', which represent the 3D points of the chessboard from real world space,
and were artificially generated (just assuming that the chessboard is placed in front of the camera at z=0).
Then, I use the method \emph{cv2.findChessboardCorners} to find the image points of chessboard corners for
the calibration images in the \emph{camera\_cal} directory.
The function \emph{cv2.cameraCalibration} was called to extract the parameters to calibrate the camera based on the collected input
from images, where all the chessboard corners were found successfully.

Based on the collected parameters, the function \emph{cv2.undistort} was used to undistort the images taken by camera.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=40mm]{../camera_cal/calibration5.jpg}
    \quad
    \includegraphics[width=40mm]{../output_images/calibrate.jpg}
  \caption{A chessboard photo, before undistortion (left) and after (right).}
\end{center}
\end{figure}

\subsection{Pipeline}

Below, I summarize my pipeline, which is executed in the function \emph{process\_image}.

\subsubsection{Provide an example of a distortion-corrected image.}

A distortion-free image from the road is provided below:

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=40mm]{../test_images/test1.jpg}
    \quad
    \includegraphics[width=40mm]{../output_images/undistorted.jpg}
  \caption{A road photo, before undistortion (left) and after (right).}
\end{center}
\end{figure}

\subsubsection{Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. 
Provide an example of a binary image result.}

After undistorting, I used a combination of color and gradient thresholds. The thresholding is performed in the master function \emph{combine\_threshold},
which calls each of the subfunctions that perform the thresholding, and overlays the resulting binary output on a single image.

Namely, I used the following thresholds:
%
\begin{itemize}
  \item Canny edge detection (function \emph{Canny\_threshold}) with $\emph{minVal}=50$ and  $\emph{maxVal}=150$;
  \item a color threshold based on the S channel in the HLS color space (function \emph{hls\_threshold}), activation for $S \in [70,255]$;
  \item a Sobel threshold, both in $x$ and $y$ directions (functions \emph{abs\_sobel\_thresh} and \emph{combine\_Sobel}), with range $[10,255]$ and $kernel=3$
    for both. The Sobel thresholds are taken in conjunction, i.e. only the pixels selected in both directions contribute to the final binary output.
\end{itemize}

I also implemented magnitude and direction thresholds (functions \emph{mag\_thresh}, \emph{dir\_threshold} and \emph{combine\_MagDir}),
but ultimately they were not included in the pipeline, as they did not bring any significant improvement.
Regardless, I will keep the implementation for future.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=40mm]{../output_images/undistorted.jpg}
    \quad
    \includegraphics[width=40mm]{../output_images/thresholded.jpg}
  \caption{The image before (left) and after (right) the thresholding operations.}
\end{center}
\end{figure}


\subsubsection{Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.}

The perspective transform was performed in two steps.
First, one of the straight lines images was used to identify four corner points to define the transform,
which were in turn used to compute the perspective transform matrix
(the function \emph{get\_perspective\_transform}, using \emph{cv2.getPerspectiveTransform}).

The second step was to apply the matrix $M$ obtained from \emph{cv2.getPerspectiveTransform} to the processed image.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=40mm]{../output_images/thresholded.jpg}
    \quad
    \includegraphics[width=40mm]{../output_images/warped.jpg}
  \caption{The image before (left) and after (right) the perspective transform.}
\end{center}
\end{figure}

\subsubsection{Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?}

In order to identify lane-line pixels, I performed the procedure of finding window centroids, as described in details in the course.
To give a rough overview, the image is divided into 9 horizontal layers. Within each layer, a window search is performed,
by convolving the binary image from the previous step, with a sliding window. 
The position, where the window yielded highest activation is selected,
and the center of each such window is later used to fit the polynomial (function \emph{find\_window\_centroids}).

Each (left/right) set of window centroids has been interpolated as a function of $Y$ (so rotated 90 degrees), with a polynomial of the form:
%
$$
 X = AY^2 + BY + C
$$

Simple reality checks were performed to stabilize the interpolation for the video (function \emph{reality\_checks}).
It is expected, that the polynomial curves will vary continuously from frame to frame,
so if the difference between the current and the previous curve (in $L^2$ norm) would exceed a given threshold,
then a replacement curve (based on previous frames) was substituted.

The thresholds were set to 600 for the left lane line and 200 for the right lane line.
The x-values for the replacement curve were computed with a power series:

$$
x_{saved}(t) = (1/3) \sum_{i} (2/3)^{i-1}x(t-i) 
$$

and stored in the \emph{saved\_line} class instance.
The coefficients in this series were chosen quite arbitrarily, but seem to have a nice smoothening effect on the video.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=40mm]{../output_images/centroids.jpg}
    \quad
    \includegraphics[width=40mm]{../output_images/line_image.jpg}
  \caption{The top activation windows (left), and their spline interpolation (right).}
\end{center}
\end{figure}



\subsubsection{Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.}

The curvature and the respective position was calculated in the function \emph{curvature\_and\_offset}.

The curves are second order polynomials, so the radius of curvature can be computed from the formula 
%
$$
  R_{curve} = \frac{(1+(2AY+B)^2)^{3/2}}{|2A|}.
$$
%
Then, I averaged from the values of the left and the right lane line.


To compute the displacement, I assumed that the camera is mounted in the middle of the car.
Then, the formula the displacement is simple, since in a centered position the left and the right line needs to be equidistant from the center,
and sum up to the width of the image:
%
$$
 \text{displacement} = (\text{image\_width} - C_{left} - C_{right})/2.
$$

In this computations, I assume I am at the bottom of the image, which in my pipeline corresponds to $Y=0$
(as contrary to $Y=Y_{max}$ in the example pipeline).
Values above zero indicate that the car is on the right side of the lane, below zero, that it is left of the center.

The calculations were performed in meters, and I assumed that each pixel consists of 60/720 meters in the y direction and 3.7/700 meters in the x direction.
In the y direction I have about twice as many meters per pixel, as recommended in the course, but this is because I choose my perspective transform
in such a way, that the lines are more squeezed in the vertical direction.
The curvature and displacement values were displayed in the upper left corner of the processed images and of the video.



\subsubsection{ Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.}

Outputs of all provided test images are displayed below:

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=40mm]{../output_images/test1_processed.jpg}
    \quad
    \includegraphics[width=40mm]{../output_images/test2_processed.jpg}
    \quad
   \includegraphics[width=40mm]{../output_images/test3_processed.jpg}
   \quad
   \includegraphics[width=40mm]{../output_images/test4_processed.jpg}
   \quad
   \includegraphics[width=40mm]{../output_images/test5_processed.jpg}
   \quad
   \includegraphics[width=40mm]{../output_images/test6_processed.jpg}
  \caption{Lane area is displayed in green.}
\end{center}
\end{figure}

\subsection{Pipeline (video)}

\subsubsection{Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video 
(wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!)}

Here is a link to the \href{https://github.com/czechows/udacity----advanced-lane-lines/blob/master/output.mp4}{\textbf{output video}}.

\subsection{Discussion}

\subsubsection{Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?}

The pipeline does not really perform well on the challenge videos.
A more structured approach to backtracing history and reality checks could have been taken (which would incorporate e.g. confidence in the result),
but given time constraints only a limited version of these was implemented.

Also, some variables could have been precomputed outside of the pipeline, which would have accelerated video processing.

\end{document}
