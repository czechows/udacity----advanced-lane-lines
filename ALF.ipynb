{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def camera_calib(filenames,nx=9,ny=6):\n",
    "    objp = np.zeros((ny*nx,3),np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2) #here we created a nice chessboard\n",
    "    \n",
    "    imgpoints = []\n",
    "    objpoints = []\n",
    "\n",
    "    for fname in filenames:\n",
    "        img = cv2.imread(fname)\n",
    "        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "        if ret == True:\n",
    "            imgpoints.append(corners)\n",
    "            objpoints.append(objp)\n",
    "\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "    return (ret, mtx, dist, rvecs, tvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undistort(img, cam_params):\n",
    "    ret, mtx, dist, rvecs, tvecs = cam_params\n",
    "    return cv2.undistort(img, mtx, dist, None, mtx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gray(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0,255)):\n",
    "    gray = convert_to_gray(img)\n",
    "\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    \n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Convert to grayscale\n",
    "    gray = convert_to_gray(img)\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= thresh[0]) & (gradmag <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "# Define a function to threshold an image for a given range and Sobel kernel\n",
    "def dir_threshold(img, sobel_kernel=9, thresh=(0, np.pi/2)):\n",
    "    # Grayscale\n",
    "    gray = convert_to_gray(img)\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def Canny_threshold(img, thresh=(0,255)):\n",
    "    # Grayscale\n",
    "    gray = convert_to_gray(img)\n",
    "    binary_output = cv2.Canny(gray,thresh[0],thresh[1])\n",
    "    binary_output[binary_output==255]=1\n",
    "    return binary_output\n",
    "    \n",
    "def hls_threshold(img, thresh=(0, 255)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    # s channel is the best for this threshold\n",
    "    s_channel = hls[:,:,2]\n",
    "    binary_output = np.zeros_like(s_channel)\n",
    "    binary_output[(s_channel > thresh[0]) & (s_channel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def combine_Sobel(img, SobelX=True, SobelY=True, SobelX_params=[], SobelY_params=[]):\n",
    "    gray = convert_to_gray(img)\n",
    "    binary_output=np.ones_like(gray)\n",
    "    \n",
    "    if(SobelX):\n",
    "        img, sobel_kernel, thresh = SobelX_params\n",
    "        binary_output = binary_output & abs_sobel_thresh(img=img, orient='x', \n",
    "                                                         sobel_kernel=sobel_kernel, thresh=thresh)\n",
    "    \n",
    "    if(SobelX):\n",
    "        img, sobel_kernel, thresh = SobelY_params\n",
    "        binary_output = binary_output & abs_sobel_thresh(img=img, orient='y', \n",
    "                                                         sobel_kernel=sobel_kernel, thresh=thresh)\n",
    "        \n",
    "    return binary_output    \n",
    "\n",
    "def combine_MagDir(img, Mag=True, Dir=True, Mag_params=[], Dir_params=[]):\n",
    "    gray = convert_to_gray(img)\n",
    "    binary_output=np.ones_like(gray)\n",
    "    \n",
    "    if(Mag):\n",
    "        img, sobel_kernel, thresh = Mag_params\n",
    "        binary_output = binary_output & abs_sobel_thresh(img=img, orient='x', \n",
    "                                                         sobel_kernel=sobel_kernel, thresh=thresh)\n",
    "    \n",
    "    if(Dir):\n",
    "        img, sobel_kernel, thresh = Dir_params\n",
    "        binary_output = binary_output & abs_sobel_thresh(img=img, orient='y', \n",
    "                                                         sobel_kernel=sobel_kernel, thresh=thresh)\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_threshold(img, Canny=True, Sobel=True, MagDir=True, HLS=True,\n",
    "                     Canny_params=[], Sobel_params=[], MagDir_params=[], HLS_params=[]):\n",
    "\n",
    "    #only for resizing\n",
    "    gray = convert_to_gray(img)  \n",
    "    binary_output=np.zeros_like(gray)\n",
    "        \n",
    "    if(Canny):\n",
    "        img, thresh = Canny_params\n",
    "        binary_output = binary_output | Canny_threshold(img, thresh)\n",
    "        canny_img = Canny_threshold(img,thresh)\n",
    "        \n",
    "    if(Sobel):\n",
    "        img, SobelX, SobelY, SobelX_params, SobelY_params = Sobel_params\n",
    "        binary_output = binary_output | combine_Sobel(img, SobelX, SobelY, SobelX_params, SobelY_params)\n",
    "        \n",
    "    if(MagDir):\n",
    "        img, Mag, Dir, Mag_params, Dir_params = MagDir_params\n",
    "        binary_output = binary_output | combine_MagDir(img, Mag, Dir, Mag_params, Dir_params)\n",
    "        \n",
    "    if(HLS):\n",
    "        img, thresh = HLS_params\n",
    "        binary_output = binary_output | hls_threshold(img,thresh)\n",
    "        hls_img = hls_threshold(img,thresh)\n",
    "\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perspective_transform(src = np.float32([[373,605], [920,605], [542, 487], [749,487]]),\n",
    "                             dst = np.float32([[373,605], [920,605], [373, 487], [920,487]])):\n",
    "    return cv2.getPerspectiveTransform(src,dst)\n",
    "\n",
    "def perspective_transform(img,M):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    return cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def mask_image(img):\n",
    "    imshape = img.shape\n",
    "    left_bottom = (100, imshape[0])\n",
    "    right_bottom = (1250, imshape[0])\n",
    "    left_top = (660, 410) \n",
    "    right_top = (640, 410)\n",
    "    in_left_bottom = (310, imshape[0])\n",
    "    in_right_bottom = (1150, imshape[0])\n",
    "    in_left_top = (700,480)\n",
    "    in_right_top = (650,480)\n",
    "    vertices = np.array([[left_bottom, left_top, right_top, right_bottom, in_right_bottom,\n",
    "                          in_left_top, in_right_top, in_left_bottom]], dtype=np.int32)\n",
    "    # Masked area\n",
    "    return region_of_interest(img, vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(image, window_width, window_height, margin):\n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(warped[int(3*warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(warped[int(3*warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(warped.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "       # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(warped[int(warped.shape[0]-(level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "\n",
    "def draw_window_centroids(img, window_width, window_height, margin):\n",
    "    window_centroids = find_window_centroids(warped, window_width, window_height, margin)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    " \n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "\n",
    "    print(img.shape)\n",
    "    print(output.shape)\n",
    "    result = cv2.addWeighted(img, 1, output, 0.3, 0)\n",
    "    plt.imshow(result)\n",
    "    \n",
    "    plt.title('window fitting results')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280)\n",
      "(720, 1280, 3)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/build/opencv/src/opencv-3.2.0/modules/core/src/arithm.cpp:659: error: (-209) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function arithm_op\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-1653e4678ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mwindow_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m \u001b[0;31m# Break image into 9 vertical layers since image height is 720\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# How much to slide left and right for searching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdraw_window_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-229-4a620aaf50b0>\u001b[0m in \u001b[0;36mdraw_window_centroids\u001b[0;34m(img, window_width, window_height, margin)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /build/opencv/src/opencv-3.2.0/modules/core/src/arithm.cpp:659: error: (-209) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function arithm_op\n"
     ]
    }
   ],
   "source": [
    "original = cv2.imread(\"test_images/test5.jpg\")\n",
    "\n",
    "Canny_params = original, (50,150)\n",
    "SobelX_params = original, 7, (10,255)\n",
    "SobelY_params = original, 7, (10,255)\n",
    "Sobel_params = original, True, True, SobelX_params, SobelY_params\n",
    "Mag_params = original, 7, (10,30)\n",
    "Dir_params = original, 7, (0.65, 1.05)\n",
    "MagDir_params = original, True, True, Mag_params, Dir_params\n",
    "HLS_params = original, (100,256)\n",
    "\n",
    "thresholded = combine_threshold(original, Canny=False, Sobel=True, MagDir=True, HLS=True,\n",
    "                               Canny_params=Canny_params, Sobel_params=Sobel_params, \n",
    "                                MagDir_params=MagDir_params, HLS_params=HLS_params)\n",
    "\n",
    "masked = mask_image(thresholded)\n",
    "M=get_perspective_transform()\n",
    "warped = perspective_transform(masked,M)\n",
    "\n",
    "#plt.subplot(121)\n",
    "#plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "#plt.subplot(122)\n",
    "#plt.imshow(warped, cmap='gray')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#plt.figure()\n",
    "#histogram = np.sum(warped[warped.shape[0]//2:,:], axis=0)\n",
    "#plt.plot(histogram)\n",
    "#plt.show()\n",
    "\n",
    "# window settings\n",
    "window_width = 50 \n",
    "window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 100 # How much to slide left and right for searching\n",
    "draw_window_centroids(warped, window_width, window_height, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
